@inproceedings{shridhar2021cliport,
  title        = {{CLIP}ort: What and Where Pathways for Robotic Manipulation},
  author       = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle    = {Proceedings of the 5th Annual Conference on Robot Learning (CoRL)},
  year         = {2021},
  url          = {https://openreview.net/forum?id=9uFiX_HRsIL},
  doi={10.48550/arXiv.2109.12098},
  abstract     = {How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP with the spatial precision (where) of Transporter. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.},
  keywords = {type:method, domain:manipulation, approach:two-stream, contribution:few-shot-generalization, interaction:vision language action, dataset:simulated, robotics_role:autonomous, fine-tuning:yes, year:2021, conference:CoRL},
}
@inproceedings{ichter2022saycan,
  title        = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  author       = {Ichter, Brian and Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and Kalashnikov, Dmitry and Levine, Sergey and Lu, Yao and Parada, Carolina and Rao, Kanishka and Sermanet, Pierre and Toshev, Alexander T. and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Yan, Mengyuan and Brown, Noah and Ahn, Michael and Cortes, Omar and Sievers, Nicolas and Tan, Clayton and Xu, Sichun and Reyes, Diego and Rettinghouse, Jarek and Quiambao, Jornell and Pastor, Peter and Luu, Linda and Lee, Kuang-Huei and Kuang, Yuheng and Jesmonth, Sally and Jeffrey, Kyle and Ruano, Rosario Jauregui and Hsu, Jasmine and Gopalakrishnan, Keerthana and David, Byron and Zeng, Andy and Fu, Chuyuan Kelly},
  booktitle    = {Proceedings of the 6th Annual Conference on Robot Learning (CoRL)},
  year         = {2022},
  url          = {https://openreview.net/forum?id=bdHkMjBJG_w},
  doi={10.48550/arXiv.2204.01691},
  abstract     = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator.},
    keywords = {type:method, domain:tool-understanding, approach:affordance-based, contribution:language grounding, interaction:vision language action, dataset:real-world, robotics_role:assistive, fine-tuning:no, year:2022, conference:CoRL},
}
@article{cresswell2018healthcare,
  title        = {Health Care Robotics: Qualitative Exploration of Key Challenges and Future Directions},
  author       = {Cresswell, Kathrin and Cunningham-Burley, Sarah and Sheikh, Aziz},
  journal      = {Journal of Medical Internet Research},
  volume       = {20},
  number       = {7},
  pages        = {e10410},
  year         = {2018},
  publisher    = {JMIR Publications},
  url          = {http://www.jmir.org/2018/7/e10410/},
  doi          = {10.2196/10410},
  pmid         = {29973336},
  abstract     = {Background: The emergence of robotics is transforming industries around the world. Robot technologies are evolving exponentially, particularly as they converge with other functionalities such as artificial intelligence to learn from their environment, from each other, and from humans. Objective: The goal of the research was to understand the emerging role of robotics in health care and identify existing and likely future challenges to maximize the benefits associated with robotics and related convergent technologies. Methods: We conducted qualitative semistructured one-to-one interviews exploring the role of robotic applications in health care contexts. Using purposive sampling, we identified a diverse range of stakeholders involved in conceiving, procuring, developing, and using robotics in a range of national and international health care settings. Interviews were digitally recorded, transcribed verbatim, and analyzed thematically, supported by NVivo 10 (QSR International) software. Theoretically, this work was informed by the sociotechnical perspective, where social and technical systems are understood as being interdependent. Results: We conducted 21 interviews and these accounts suggested that there are significant opportunities for improving the safety, quality, and efficiency of health care through robotics, but our analysis identified 4 major barriers that need to be effectively negotiated to realize these: (1) no clear pull from professionals and patients, (2) appearance of robots and associated expectations and concerns, (3) disruption of the way work is organized and distributed, and (4) new ethical and legal challenges requiring flexible liability and ethical frameworks. Conclusions: Sociotechnical challenges associated with the effective integration of robotic applications in health care settings are likely to be significant, particularly for patient-facing functions. These need to be identified and addressed for effective innovation and adoption.},
keywords = {type:application, domain:healthcare, approach:qualitative_study, contribution:challenge_identification, interaction:robot-human, dataset:n/a, robotics_role:assistive, fine-tuning:no, year:2018, conference:JMIR},
}
@article{lynch2023ilang,
  title        = {Interactive Language: Talking to Robots in Real Time},
  author       = {Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
  journal      = {IEEE Robotics and Automation Letters},
  year         = {2023},
  volume       = {},
  number       = {},
  pages        = {1--8},
  doi          = {10.1109/LRA.2023.3295255},
  url          = {https://interactive-language.github.io},
  abstract     = {We present a framework for building interactive, real-time, natural language-instructable robots in the real world, and we open source related assets (dataset, environment, benchmark, and policies). Trained with behavioral cloning on a dataset of hundreds of thousands of language-annotated trajectories, a produced policy can proficiently execute an order of magnitude more commands than previous works: specifically we estimate a 93.5% success rate on a set of 87,000 unique natural language strings specifying raw end-to-end visuolinguo-motor skills in the real world. We find that the same policy is capable of being guided by a human via real-time language to address a wide range of precise long-horizon rearrangement goals, e.g. “make a smiley face out of blocks”. The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets. We hope the demonstrated results and associated assets enable further advancement of helpful, capable, natural-language-interactable robots.},
 keywords = {type:method, domain:manipulation, approach:behavior_cloning, contribution:real-time, interaction:vision language action, dataset:real-world, robotics_role:autonomous, fine-tuning:no, year:2023, conference:LRA},
}
@article{xu2023survey,
  title        = {Multimodal Learning With Transformers: A Survey},
  author       = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year         = {2023},
  volume       = {45},
  number       = {10},
  pages        = {12113--12132},
  doi          = {10.1109/TPAMI.2023.3275156},
  abstract     = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
  keywords = {type:survey, domain:general, approach:transformer, contribution:taxonomy, interaction:vision language action, dataset:multiple, robotics_role:n/a, fine-tuning:no, year:2023, conference:TPAMI},
}
@inproceedings{kim2024openvla,
  title        = {Open{VLA}: An Open-Source Vision-Language-Action Model},
  author       = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan P and Sanketi, Pannag R and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  booktitle    = {Proceedings of the 8th Annual Conference on Robot Learning (CoRL)},
  year         = {2024},
  url          = {https://openreview.net/forum?id=ZMnD6QZAE6},
  doi={10.48550/arXiv.2406.09246},
  abstract     = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as (1) existing VLAs are largely closed and inaccessible to the public, and (2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, where we outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  keywords = {type:method, domain:manipulation, approach:transformer, contribution:open-source, interaction:vision language action, dataset:real-world, robotics_role:generalist, fine-tuning:yes, year:2024, conference:CoRL},
}
@inproceedings{shridhar2022pactor,
  title        = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation},
  author       = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle    = {Proceedings of the 6th Annual Conference on Robot Learning (CoRL)},
  year         = {2022},
  url          = {https://openreview.net/forum?id=PS_eCS_WCvD},
  doi={10.48550/arXiv.2209.05451},
  abstract     = {Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can manipulation still benefit from Transformers with the right problem formulation? We investigate this question with PerAct, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PerAct encodes language goals and RGB-D voxel observations with a Perceiver Transformer, and outputs discretized actions by “detecting the next best voxel action”. Unlike frameworks that operate on 2D images, the voxelized 3D observation and action space provides a strong structural prior for efficiently learning 6-DoF actions. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PerAct significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.},
  keywords = {type:method, domain:manipulation, approach:transformer, contribution:multi-task-policy, interaction:vision language action, dataset:simulated, robotics_role:autonomous, fine-tuning:yes, year:2022, conference:CoRL},
}
@inproceedings{brohan2023rt1,
  title        = {RT-1: Robotics Transformer for Real-World Control at Scale},
  author       = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Zitkovich, Brianna},
  booktitle    = {Proceedings of Robotics: Science and Systems (RSS)},
  year         = {2023},
  month        = {July},
  doi          = {10.15607/RSS.2023.XIX.025},
  abstract     = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.},
 keywords = {type:method, domain:manipulation, approach:transformer, contribution:scalable_model, interaction:vision language action, dataset:real-world, robotics_role:autonomous, fine-tuning:no, year:2023, conference:RSS},
}
@inproceedings{zitkovich2023rt2,
  title        = {RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
  author       = {Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and Vuong, Quan and Vanhoucke, Vincent and Tran, Huong and Soricut, Radu and Singh, Anikait and Singh, Jaspiar and Sermanet, Pierre and Sanketi, Pannag R. and Salazar, Grecia and Ryoo, Michael S. and Reymann, Krista and Rao, Kanishka and Pertsch, Karl and Mordatch, Igor and Michalewski, Henryk and Lu, Yao and Levine, Sergey and Lee, Lisa and Lee, Tsang-Wei Edward and Leal, Isabel and Kuang, Yuheng and Kalashnikov, Dmitry and Julian, Ryan and Joshi, Nikhil J. and Irpan, Alex and Ichter, Brian and Hsu, Jasmine and Herzog, Alexander and Hausman, Karol and Gopalakrishnan, Keerthana and Fu, Chuyuan and Florence, Pete and Finn, Chelsea and Dubey, Kumar Avinava and Driess, Danny and Ding, Tianli and Choromanski, Krzysztof Marcin and Chen, Xi and Chebotar, Yevgen and Carbajal, Justice and Brown, Noah and Brohan, Anthony and Arenas, Montserrat Gonzalez and Han, Kehang},
  booktitle    = {Proceedings of the 7th Conference on Robot Learning (CoRL)},
  series       = {Proceedings of Machine Learning Research},
  volume       = {229},
  pages        = {2165--2183},
  year         = {2023},
  month        = {Nov},
  publisher    = {PMLR},
  url          = {https://proceedings.mlr.press/v229/zitkovich23a.html},
  pdf          = {https://proceedings.mlr.press/v229/zitkovich23a/zitkovich23a.pdf},
  doi={10.48550/arXiv.2307.15818},
  abstract     = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
keywords = {type:method, domain:manipulation, approach:transformer, contribution:semantic_reasoning, interaction:vision language action, dataset:internet-scale, robotics_role:autonomous, fine-tuning:yes, year:2023, conference:CoRL},
}

